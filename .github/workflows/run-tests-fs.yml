name: Feature and Training Integration Tests for my-mlops-project
on:
  workflow_dispatch:
  pull_request:
    paths-ignore:
      - 'databricks-config/**'

env:
  DATABRICKS_HOST: https://dbc-38ce632c-4934.cloud.databricks.com
  NODE_TYPE_ID: i3.xlarge
  DATABRICKS_TOKEN: ${{secrets.STAGING_WORKSPACE_TOKEN}}

concurrency: my-mlops-project-feature-training-integration-test-staging

jobs:
  unit_tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions/setup-python@v2
        with:
          python-version: 3.8
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r test-requirements.txt
      - name: Run tests with pytest
        run: pytest
  
  integration_test:
    needs: unit_tests
    runs-on: ubuntu-20.04
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3
      # This step populates a JSON Databricks job payload that will be submitted as an integration test run.
      # It currently builds a one-off multi-task job that contains feature engineering tasks to populate Feature
      # Store tables, and a training task that uses those tables. 
      # You will need to modify the contents below to fit your pipelines (both # of tasks and input parameters for each
      # task).
      - name: Build JSON job payload for integration test
        uses: actions/github-script@v6
        id: integration-test-content
        with:
          # TODO update the tasks and notebook parameters below to match your integration test setup.
          script: |
            const output = `
                    {
            "run_name": "features-training-integration-test",
            "tasks": [
              {
                "task_key": "pickup-features",
                "notebook_task": {
                  "notebook_path": "notebooks/GenerateAndWriteFeatures", 
                  "base_parameters": {
                    "test_mode": "True",
                    "input_table_path": "/databricks-datasets/nyctaxi-with-zipcodes/subsampled",
                    "timestamp_column": "tpep_pickup_datetime",
                    "output_table_name": "feature_store_taxi_example.trip_pickup_features_test",
                    "features_transform_module": "pickup_features",
                    "primary_keys": "zip"
                  }
                },
                "new_cluster": {
                  "spark_version": "11.0.x-cpu-ml-scala2.12",
                  "node_type_id": "${{ env.NODE_TYPE_ID }}",
                  "num_workers": 0,
                  "spark_conf": {
                    "spark.databricks.cluster.profile": "singleNode",
                    "spark.master": "local[*, 4]"
                  },
                  "custom_tags": {
                    "ResourceClass": "SingleNode",
                    "clusterSource": "mlops-stack/0.0"
                  }
                }
              },
              {
                "task_key": "dropoff-features",
                "notebook_task": {
                  "notebook_path": "notebooks/GenerateAndWriteFeatures",
                  "base_parameters": {                    
                    "test_mode": "True",
                    "input_table_path": "/databricks-datasets/nyctaxi-with-zipcodes/subsampled",
                    "timestamp_column": "tpep_dropoff_datetime",
                    "output_table_name": "feature_store_taxi_example.trip_dropoff_features_test",
                    "features_transform_module": "dropoff_features",
                    "primary_keys": "zip"
                  }
                },
                "new_cluster": {
                  "spark_version": "11.0.x-cpu-ml-scala2.12",
                  "node_type_id": "${{ env.NODE_TYPE_ID }}",
                  "num_workers": 0,
                  "spark_conf": {
                    "spark.databricks.cluster.profile": "singleNode",
                    "spark.master": "local[*, 4]"
                  },
                  "custom_tags": {
                    "ResourceClass": "SingleNode",
                    "clusterSource": "mlops-stack/0.0"
                  }
                }
              },
              {
                "task_key": "training",
                "depends_on": [
                  {
                    "task_key": "dropoff-features"
                  },
                  {
                    "task_key": "pickup-features"
                  }
                ],
                "notebook_task": {
                  "notebook_path": "notebooks/TrainWithFeatureStore",
                  "base_parameters": {
                    "test_mode": "True",
                    "training_data_path": "/databricks-datasets/nyctaxi-with-zipcodes/subsampled",
                    "experiment_name": "/Shared/my-mlops-project/my-mlops-project-experiment-test",
                    "model_name": "my-mlops-project-model-test",
                    "fs_stage": "_test"
                  }
                },
                "new_cluster": {
                  "spark_version": "11.0.x-cpu-ml-scala2.12",
                  "node_type_id": "${{ env.NODE_TYPE_ID }}",
                  "num_workers": 0,
                  "spark_conf": {
                    "spark.databricks.cluster.profile": "singleNode",
                    "spark.master": "local[*, 4]"
                  },
                  "custom_tags": {
                    "ResourceClass": "SingleNode",
                    "clusterSource": "mlops-stack/0.0"
                  }
                }
              }
            ],
            "git_source": {
              "git_url": "${{ github.server_url }}/${{ github.repository }}",
              "git_provider": "gitHub",
              "git_commit": "${{ github.event.pull_request.head.sha || github.sha }}"
            },
            "access_control_list": [
              {
                "group_name": "users",
                "permission_level": "CAN_VIEW"
              }
            ]
            }`
            return output.replace(/\r?\n|\r/g, '')    
      - name: Feature Store/Model Training Integration Test
        id: features-training-integration-test
        run: |
          python -m pip install --upgrade pip
          pip install databricks-cli
          databricks jobs configure --version=2.1
          echo ${{steps.integration-test-content.outputs.result}} > test.json 
          databricks runs submit --json-file test.json --wait > tmp-output.json
          # We want to extract the run id as it's useful to show in the Github UI (as a comment).
          head -3  tmp-output.json  | jq '.run_id'  > run-id.json
          databricks runs get --run-id "$(cat run-id.json)" | jq -r '.run_page_url' > run-page-url.json
          echo "run-url=$(cat run-page-url.json)" >> "$GITHUB_OUTPUT"
      #- name: Create Comment with Training Model Output
      #  uses: actions/github-script@v6
      #  id: comment
      #  with:
      #    github-token: ${{ secrets.GITHUB_TOKEN }}
      #    script: |
      #      const output = `
      #      The training integration test run is available [here](${{ steps.features-training-integration-test.outputs.run-url }}).`
#
      #      github.rest.issues.createComment({
      #        issue_number: context.issue.number,
      #        owner: context.repo.owner,
      #        repo: context.repo.repo,
      #        body: output
      #      })
